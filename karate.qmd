---
title: "Bayesian community detection"
subtitle: "Random graphs and network statistics"
description: | 
  A presentation on the Bayesian treatment of community detection
thanks: \url{https://github.com/yannmclatchie/karate}
author:
  - name: Yann McLatchie
    institute: "Department of computer science, Aalto University"
    url: https://yannmclatchie.github.io/
    affiliation: Department of Computer Science, Aalto University
citation:
  url: https://github.com/yannmclatchie/karate

format: beamer
editor: visual
bibliography: community.bib
---

## Community detection

Suppose we observe an adjacency matrix $A = (A_{ij})$ of a graph, and task to infer the community memberships of each node $(z_i),\,i=1,\dots,n$. One way to do this is to model the structure of the graph, and specifically model $A \overset{d}=\text{SBM}(z, P)$ with the link probability matrix $P$ also unobserved.

## Aim

@vdpas2018 task to produce a consistent Bayesian estimator of the community structure for an SBM given a fixed number of communities.

## Consistency

An estimator $\bar{X}_n$ of a random variable $X$ is deemed _weakly consistent_ (partial recovery) if it converges _in probability_ to the true value of the variable $X^\ast$,
$$
\lim_{n\to\infty}\mathbb{P}(|\bar{X}_n - X^\ast|>\epsilon) = 0,\,\forall\epsilon>0.
$$

An estimator $\bar{X}_n$ of a random variable $X$ is deemed _strongly consistent_ (exact recovery) if it converges _almost surely_ to the true value of the variable $X^\ast$,
$$
\mathbb{P}(\lim_{n\to\infty}\bar{X}_n = X^\ast) = 1.
$$

## Community structure in the SBM

We have an undirected random graph $G$ on $n$ nodes, each belonging to one of $K\in\mathbb{N}$ classes. Each node is randomly labelled according to i.i.d. $Z_1,\dots,Z_n$ random variables with probability $\pi_1,\dots,\pi_K$. Given this set of labels, edges between nodes are independently sampled from a Bernoulli random variable dependent on the label, $\mathbb{P}(A_{ij} = 1 \mid Z) = P_{Z_i,Z_j}$. 

The likelihood of our SBM is then defined as
$$
\prod_{1\leq i < j\leq n}P_{Z_i,Z_j}^{A_{ij}}(1 - P_{Z_i,Z_j})^{1 - A_{ij}}\prod_{1\leq i\leq n}\pi_{Z_i}.
$$

## Bayesian inference

We wish to infer parameter $\theta \in \Theta$ over which we have prior information $p(\theta)$. We achieve a \textit{posterior} belief $p(\theta \mid x_{1:n})$ by combining our prior with a likelihood $p(x_{1:n} \mid \theta)$ and by performing the belief update [@bernardo2009bayesian].
$$
p(\theta \mid x_{1:n}) \propto p(x_{1:n} \mid \theta) p(\theta).
$$

## Prior choices

$$
\begin{aligned}
\pi&\sim\textsf{Dirichlet}(\alpha,\dots,\alpha) \\
P_{i,j}&\overset{\text{i.i.d.}}\sim\textsf{Beta}(\beta_1, \beta_2) \\
e_i\mid\pi, P&\sim\pi\\
A_{ij}\mid\pi, P, e&\sim\textsf{Bernoulli}(P_{e_i, e_j})
\end{aligned}
$$
(Hyper-priors over $\alpha, \beta_1, \beta_2$ also available, and not very sensitive, can use for instance $\alpha = 0.5$ and $\beta_1 = \beta_2 = 0.5$).

## The posterior

@vdpas2018 call the posterior class distribution $p(e\mid A)$ the _Bayesian modularity_, denoted $Q_B(e)$, and we then assign class labels according to
$$
\hat{e} = \arg\max_e Q_B(e).
$$
As such, we classify nodes into community based on the maximum _a posteriori_ (MAP) estimate of $e$. The Bayesian modularity is connected to the _likelihood modularity_ of @bickel2009, in that the latter exists as a special case of the former.

## Why be Bayesian?

- Computationally efficiency of Gibbs sampler compared to maximum likelihood for large $n$
- Complete posterior predictive distribution
  - Uncertainty quantification
  - Decision analysis
- Ability to encode prior beliefs
- "A Bayesian version will usually make things better." [@gelmanquotes]

## The main result

\begin{theorem}
  Denote $\rho_n = \sum_{i,j}\pi_i\pi_jP_{i, j}$, then:
  
  1. if $(P, \pi)$ is fixed and identifiable ($\pi$ has strictly positive coordinates, and rows of $P$ are distinct) then the MAP estimator $\hat{e}$ is strongly consistent;
  
  2. if $P = \rho_nS$ with $(S, \pi)$ is fixed and identifiable then the MAP estimator $\hat{e}$ is strongly consistent if $(n-1)\rho_n \gg (\log n)^2$, where $\mathbb{E}[\text{deg}_G(i)] = (n-1)\rho_n$.
\end{theorem}

## How much data is enough data?

$(n-1)\rho_n\gg\log n$ is sufficient for weakly consistent community detection [@lei2015].

@bickel2009 claim the likelihood modularity is strongly consistent for arbitrary $K$ under $(n-1)\rho_n\gg\log n$. **However**, this was shown under the assumption that the modularity is globally Lipschitz, which is not the case in general.

$(n-1)\rho_n\gg(\log n)^2$ is sufficient for the likelihood (and thus also the Bayesian) modularity to be strongly consistent for arbitrary $K$. In the special case $K=2$, $(n-1)\rho_n\gg\log n$ is also sufficient.

## An application in Stan: Zachary's karate club

```{r}
#| echo: false
#| fig-cap: "The karate club graph, with empirical average degree ~4.6 and log(n) ~3.5."
#| label: "fig-karate"
#| fig-width: 2.5
#| fig-asp: 1

library(igraph)
library(ggplot2)

# set a seed for reproducibility
set.seed(210426)

# read the karate graph
karate <- make_graph("Zachary")

# plot the karate club graph
par(mar=c(0,0,0,0)+.1)
plot(karate, layout = layout_in_circle) 
```

## The model in Stan

This code is adapted from @Sarkar2018.
```
model {
  for(i in 1:K){
    for(j in 1:K){
      // prior over kernel matrix
      phi[i][j] ~ beta(beta[1], beta[2]);
    }
  }
  // prior over community distribution
  pi ~ dirichlet(alpha);
  for(i in 1:N){
    for(j in i+1:N){ //symmetry and ignore diagonals
      // likelihood
      graph[i][j] ~ bernoulli(pi' * phi * pi); 
    }
  }
}
```

## The fitted model 

```{r}
#| output: false
#| warning: false

# convert the graph into an adjacency matrix
adj <- as_adj(karate, sparse = FALSE)

# define model data
data <- list(
  N = vcount(karate), 
  K = 2, 
  beta = c(0.5, 0.5), 
  alpha = rep(1 / 2, 2), 
  graph = adj
)

# fit Bayesian SBM model in Stan
model <- rstan::stan_model(file = "sbm.stan")
fit <- rstan::sampling(model, data = data, seed = 170899)
```

```{r}
#| echo: false
#| fig-cap: "Posterior SBM parameters."
#| label: "fig-posterior"
#| fig-width: 2.5
#| fig-asp: 1

bayesplot::mcmc_areas(
  fit, 
  pars = c("phi[1,1]", "phi[1,2]", "phi[2,1]", "phi[2,2]", "pi[1]", "pi[2]")
)
```

## Decision analysis


```{r}
#| echo: false
#| fig-cap: "Posterior predictive distribution over six individual node communities."
#| label: "fig-posterior-predictive"
#| fig-width: 2.5
#| fig-asp: 1

bayesplot::mcmc_areas(
  fit, 
  pars = c(
    "clusters_inf[1]", 
    "clusters_inf[2]", 
    "clusters_inf[3]", 
    "clusters_inf[4]", 
    "clusters_inf[5]",
    "clusters_inf[6]"
  )
)
```

## Prediction

```{r}
#| echo: false
#| fig-cap: "MAP cluster predictions from the Bayesian modularity. Shape and colour of node show predicted class, while coloured clouds indicate true communities."
#| label: "fig-prediction"
#| fig-width: 2.5
#| fig-asp: 1

# extract community membership
fit_ss <- rstan::extract(fit, permuted = TRUE)
membership <- apply(fit_ss$clusters_inf, 2, median)
communities <- list("1" = which(membership == 1), "2" = which(membership == 2))
shapes <- ifelse(membership == 1, "circle", "square")
colours <- ifelse(membership == 1, "lightyellow", "lightgreen")

# define true memberships
true_membership <- c(1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 
                     2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)
true_communities <- list(
  "1" = which(true_membership == 1), 
  "2" = which(true_membership == 2)
)

# plot clusters
par(mar=c(0,0,0,0)+.1)
plot(karate, 
     mark.groups = true_communities,
     vertex.shape = shapes,
     vertex.color = colours)
```

## Prior and likelihood sensitivity

```{r}
#| echo: false
#| fig-cap: "Prior and likelihood sensitivity of posterior."
#| label: "fig-priorsense"
#| fig-height: 3
#| fig-width: 7.5

# prior sensitivity
pss <- priorsense::powerscale_sequence(fit)
priorsense::powerscale_plot_ecdf(
  pss, 
  variables = c(
    "phi[1,1]", "phi[1,2]", "phi[2,1]", "phi[2,2]", "pi[1]", "pi[2]"
  )
) + theme_classic() + theme(title = element_blank())
```

## References {.allowframebreaks}
